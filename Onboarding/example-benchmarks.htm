<!DOCTYPE html>
<html lang="en">

<head>
    <title>Example Benchmarks for LLMs</title>
    <meta charset="utf-8" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Fira Code&amp;family=Outfit:wght@400;700&amp;family=Merriweather:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap" />
    <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="og:site_name" content="hotpotqa" />
    <meta property="og:title" content="Example Benchmarks for LLMs" />
    <meta property="og:type" content="website" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Example Benchmarks for LLMs" />
    <meta name="twitter:description"
        content="As you will have realised by now, there are many examples of benchmarks in the world of LLMs that test models on a variety of tasks. We will discuss some examples that more relevant to the work we are doing on this project." />
    <meta property="og:description"
        content="As you will have realised by now, there are many examples of benchmarks in the world of LLMs that test models on a variety of tasks. We will discuss some examples that more relevant to the work we are doing on this project." />
    <meta property="og:image:type" content="image/webp" />
    <meta property="og:image:alt"
        content="As you will have realised by now, there are many examples of benchmarks in the world of LLMs that test models on a variety of tasks. We will discuss some examples that more relevant to the work we are doing on this project." />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png" />
    <meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png" />
    <meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png" />
    <meta property="twitter:domain" content="quartz.jzhao.xyz" />
    <meta property="og:url" content="https://quartz.jzhao.xyz/Onboarding/example-benchmarks" />
    <meta property="twitter:url" content="https://quartz.jzhao.xyz/Onboarding/example-benchmarks" />
    <link rel="icon" href="../static/icon.png" />
    <meta name="description"
        content="As you will have realised by now, there are many examples of benchmarks in the world of LLMs that test models on a variety of tasks. We will discuss some examples that more relevant to the work we are doing on this project." />
    <meta name="generator" content="Quartz" />
    <link href="../index.css" rel="stylesheet" type="text/css" spa-preserve />
    <link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css"
        spa-preserve />
    <script src="../prescript.js" type="application/javascript" spa-preserve></script>
    <script type="application/javascript"
        spa-preserve>const fetchData = fetch("../static/contentIndex.json").then(data => data.json())</script>
</head>

<body data-slug="Onboarding/example-benchmarks">
    <div id="quartz-root" class="page">
        <div id="quartz-body">
            <div class="left sidebar">
                <h2 class="page-title"><a href="../hotpotqa_netlify_default.html">hotpotqa</a></h2>
                <div class="spacer mobile-only"></div>
                <div class="search"><button class="search-button" id="search-button">
                        <p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7">
                            <title>Search</title>
                            <g class="search-path" fill="none">
                                <path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path>
                                <circle cx="8" cy="8" r="7"></circle>
                            </g>
                        </svg>
                    </button>
                    <div id="search-container">
                        <div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text"
                                aria-label="Search for something" placeholder="Search for something" />
                            <div id="search-layout" data-preview="true"></div>
                        </div>
                    </div>
                </div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg"
                        xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px"
                        viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve"
                        aria-label="Dark mode">
                        <title>Dark mode</title>
                        <path
                            d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z">
                        </path>
                    </svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
                        version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100"
                        style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode">
                        <title>Light mode</title>
                        <path
                            d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z">
                        </path>
                    </svg></button>
                <div class="explorer desktop-only"><button type="button" id="explorer" data-behavior="collapse"
                        data-collapsed="collapsed" data-savestate="true"
                        data-tree="[{&quot;path&quot;:&quot;Onboarding&quot;,&quot;collapsed&quot;:true}]"
                        aria-controls="explorer-content" aria-expanded="false">
                        <h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14"
                            viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round" class="fold">
                            <polyline points="6 9 12 15 18 9"></polyline>
                        </svg>
                    </button>
                    <div id="explorer-content">
                        <ul class="overflow" id="explorer-ul">
                            <li>
                                <div class="folder-outer open">
                                    <ul style="padding-left:0;" class="content" data-folderul>
                                        <li>
                                            <div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg"
                                                    width="12" height="12" viewBox="5 8 14 8" fill="none"
                                                    stroke="currentColor" stroke-width="2" stroke-linecap="round"
                                                    stroke-linejoin="round" class="folder-icon">
                                                    <polyline points="6 9 12 15 18 9"></polyline>
                                                </svg>
                                                <div data-folderpath="Onboarding"><button class="folder-button"><span
                                                            class="folder-title">Getting Started</span></button></div>
                                            </div>
                                            <div class="folder-outer ">
                                                <ul style="padding-left:1.4rem;" class="content"
                                                    data-folderul="Onboarding">
                                                    <li><a href="https://hotpotqa.netlify.app/Onboarding/intro-to-benchmarking"
                                                            data-for="Onboarding/intro-to-benchmarking">A Brief
                                                            Introduction to Benchmarking</a></li>
                                                    <li><a href="https://hotpotqa.netlify.app/Onboarding/example-benchmarks"
                                                            data-for="Onboarding/example-benchmarks">Example Benchmarks
                                                            for LLMs</a></li>
                                                    <li><a href="https://hotpotqa.netlify.app/Onboarding/homework-task"
                                                            data-for="Onboarding/homework-task">Homework?!?</a></li>
                                                </ul>
                                            </div>
                                        </li>
                                        <li>
                                            <div class="folder-outer ">
                                                <ul style="padding-left:0;" class="content" data-folderul></ul>
                                            </div>
                                        </li>
                                        <li><a href="../about.htm" data-for="about">About hotpotqa</a></li>
                                        <li><a href="../roadmap.htm" data-for="roadmap">Project Roadmap</a></li>
                                    </ul>
                                </div>
                            </li>
                            <li id="explorer-end"></li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="center">
                <div class="page-header">
                    <div class="popover-hint">
                        <nav class="breadcrumb-container" aria-label="breadcrumbs">
                            <div class="breadcrumb-element"><a href="../hotpotqa_netlify_default.html">Home</a>
                                <p> ❯ </p>
                            </div>
                            <div class="breadcrumb-element"><a href="https://hotpotqa.netlify.app/Onboarding/">Getting
                                    Started</a>
                                <p> ❯ </p>
                            </div>
                            <div class="breadcrumb-element">
                                <a "https://hotpotqa.netlify.app/onboarding/href">Example Benchmarks for LLMs</a>
                            </div>
                        </nav>
                        <h1 class="article-title">Example Benchmarks for LLMs</h1>
                        <p show-comma="true" class="content-meta"><time datetime="2025-01-31T23:10:11.641Z">Jan 31,
                                2025</time><span>12 min read</span></p>
                    </div>
                </div>
                <article class="popover-hint">
                    <p>As you will have realised by now, there are many examples of benchmarks in the world of LLMs that
                        test models on a variety of tasks. We will discuss some examples that more relevant to the work
                        we are doing on this project. The two papers we will look at are:</p>
                    <ul>
                        <li><a href="https://arxiv.org/abs/2310.06770" class="external">SWE-Bench<svg aria-hidden="true"
                                    class="external-icon" style="max-width:0.8em;max-height:0.8em;"
                                    viewBox="0 0 512 512">
                                    <path
                                        d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z">
                                    </path>
                                </svg></a></li>
                        <li><a href="https://arxiv.org/abs/2406.11612" class="external">Long Code Arena<svg
                                    aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;"
                                    viewBox="0 0 512 512">
                                    <path
                                        d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z">
                                    </path>
                                </svg></a></li>
                    </ul>
                    <p>We will breakdown each benchmark paper using the framework we have defined in our <a
                            href="https://hotpotqa.netlify.app/Onboarding/intro-to-benchmarking" class="internal alias"
                            data-slug="Onboarding/intro-to-benchmarking">brief introduction</a>
                        i.e. we will look at the following: task selection, dataset creation, and evaluation metrics
                        with some minor discussion on the experimentation side of things as well. I would highly
                        recommend having a read of the papers for the smaller (and equally important) details. These
                        papers focus mainly on coding tasks as that is what this project targets. However, reading
                        benchmark methods for general natural language tasks in long context would be good practice as
                        some of the techniques could be applied here, particularly in how the input to the model is
                        processed (<em>c.f.</em> <a href="https://arxiv.org/abs/2308.14508" class="external">Long Bench:
                            A Bilingual, Multitask Benchmark for Long Context Understanding<svg aria-hidden="true"
                                class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512">
                                <path
                                    d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z">
                                </path>
                            </svg></a>)</p>
                    <h2 id="swe-bench-can-language-models-resolve-real-world-github-issues">SWE-bench: Can Language
                        Models Resolve Real-World GitHub Issues?<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true"
                            href="example-benchmarks.htm#swe-bench-can-language-models-resolve-real-world-github-issues"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h2>
                    <h3 id="1-task-selection">1. Task Selection<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#1-task-selection" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <p>There is a single task for SWE-bench which is: <em>“to generate a pull request that addresses a
                            given issue and passes test related to the issue”</em> and is shown in the figure below.
                        <img src="../_assets/swe-bench-task.png" width="auto" height="auto" alt />
                    </p>
                    <p>The model is being tested beyond simple code generation---it needs to generate code revisions in
                        multiple locations within a large codebase. In this way, the benchmark tests an LLMs ability to
                        work with code in a realistic software engineering setting.</p>
                    <h3 id="2-dataset-creation">2. Dataset Creation<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#2-dataset-creation"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <p>The benchmark dataset is curated by scraping data from 12 open-source Python repositories on
                        GitHub. The repositories are selected based on criteria including being popular,
                        well-maintained, and having good test coverage. The pull requests (PRs) from these are scraped
                        resulting in around 90,000 PRs.</p>
                    <p>Next, a series of filtering is carried out to curate a set of suitable “task instances” for the
                        dataset. In other words, only the PRs that can be used for the defined task are selected. The
                        first filter selects PRs that resolve a GitHub issue and also make changes to the test files of
                        the repository. The second filter is “execution-based” which compares the PR’s test results
                        before and after the PR’s changes are applied. If the task instance passes all test and has no
                        installation or runtime errors, it is accepted into the dataset.</p>
                    <p>The choice of these filtering techniques is to ensure the quality of data is high and useful to
                        test for the chosen task. After filtering, the 90,000 PRs scraped initially are reduced to
                        <strong>2,294 task instances</strong> include in SWE-bench consisting of the issue text (the
                        problem), a complete codebase snapshot (context), and tests (to check output).
                    </p>
                    <h3 id="3-evaluation-metrics">3. Evaluation Metrics<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#3-evaluation-metrics"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <p>The metric used to assess model performance is the percentage of task instances that are
                        resolved. Given the code and the problem, the model generates a patch in an attempt to resolve
                        the issue adds it to the codebase. If all tests pass, then the task instance is considered
                        resolved.
                        <img src="../_assets/swe-bench-eval.png" width="auto" height="auto" alt />
                    </p>
                    <h3 id="experiments">Experiments<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#experiments" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <p>The paper discusses steps to construct an input to a model for running the SWE-bench evaluation.
                        Since the input to the models can be exceedingly long (a codebase had over 400,000 lines on
                        average), additional techniques had to be employed to fit the input into a model’s context
                        window. This involved using retrieval methods (BM25 and “Oracle” retrieval) to provide context
                        to the model. While detailed discussion will not be given here about these methods, it should
                        give you an idea of the types of considerations that need to be made when designing experiments
                        and conducting evaluation using the benchmark you have designed.</p>
                    <h2 id="long-code-arena-a-set-of-benchmarks-for-long-context-code-models">Long Code Arena: a Set of
                        Benchmarks for Long-Context Code Models<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true"
                            href="example-benchmarks.htm#long-code-arena-a-set-of-benchmarks-for-long-context-code-models"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h2>
                    <p>Note that for the sake of brevity, many of the minor details have been omitted herein. Further
                        information of each step of this process can be found in the associated paper.</p>
                    <h3 id="1-task-selection-1">1. Task Selection<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#1-task-selection-1"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <p>Long Code Arena is a suite of benchmarks that covers six different tasks in “Machine Learning for
                        Software Engineering”. These tasks are:</p>
                    <h4 id="library-based-code-generation">Library-based code generation:<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#library-based-code-generation" class="internal"><svg width="18"
                                height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                                stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Given a task description and access to the software library, the model must generate a single
                        file that solves the task using the existing methods from the library.</p>
                    <h4 id="ci-builds-repair">CI Builds Repair:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#ci-builds-repair" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The model is tasked to generate a patch to solve a real-life issue in a failing Continuous
                        Integration (CI) setup. It is given a snapshot of the repository at the commit that caused the
                        CI failure and associated logs of the failed step.</p>
                    <h4 id="project-level-code-completion">Project-Level Code Completion:<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#project-level-code-completion" class="internal"><svg width="18"
                                height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                                stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Given relevant information from the project and a prefix of a completion file, the model needs to
                        generate the next line in this file.</p>
                    <h4 id="commit-message-generation">Commit Message Generation:<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true" href="example-benchmarks.htm#commit-message-generation"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The model is tasked with generating a commit message for a given single large commit. It is
                        provided with the project files and diffs.</p>
                    <h4 id="bug-localisation">Bug Localisation:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#bug-localisation" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The model needs to identify the correct files in the codebase that need to be modified to resolve
                        a given bug description.</p>
                    <h4 id="module-summarization">Module Summarization:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#module-summarization"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The model should generate a module’s documentation when given the module’s source code and a
                        description of the expected documentation.</p>
                    <h3 id="2-dataset-creation-1">2. Dataset Creation<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#2-dataset-creation-1"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <p>Each task in this benchmark requires its own dataset which are mostly built from open-source
                        GitHub repositories. As such, there is an initial collection of repositories that are scraped
                        and filtered which are then subjected to additional filtering and processing for each specific
                        task.</p>
                    <p>The initial collection find repositories using GitHub search that pass the following filters:</p>
                    <ul>
                        <li>at least 1,000 commits</li>
                        <li>at least 10,000 lines of code</li>
                        <li>not a fork</li>
                        <li>last commit after 01/06/2023</li>
                    </ul>
                    <p>A total of 4,343 repositories are identified and downloaded via the GitHub API along with
                        corresponding issues and pull request data. Task-specific filters are applied to prepare
                        evaluation datasets for each of the six tasks. These are described in the following:</p>
                    <h4 id="library-based-code-generation-1">Library-based code generation<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#library-based-code-generation-1" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Usage examples of the libraries method are extracted from Python projects by searching for a
                        “examples” directory. These usage examples are filtered based on their length (100 ≤ number of
                        characters ≤ 40,000), length of comments (must have ≥ 400 characters of comments), and number of
                        library-specific API calls (must have ≥ 10 calls). As a result, 150 usage examples are derived
                        from 62 libraries. Each example is then provided as an input to GPT-4, prompted to create an
                        instruction that generates the given example, such that instruction-example pairs are created
                        which are manually verified. Benchmark users have access to the content of the libraries used in
                        this task meaning that they can also provide context for this generation task.</p>
                    <h4 id="ci-builds-repair-1">CI Builds Repair<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#ci-builds-repair-1"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>For this task, GitHub Action logs are also retrieved for the 100 largest downloaded Python
                        repositories during the initial collection. Pairs of consecutive GitHub action runs, where the
                        first commit caused a failure and the next was successful, are collected. The logs of the failed
                        step, the diff between the failed and successful commits, and meta-information of the failed
                        commit are collected. Runs are filtered if they take more than ten minutes, require workflows
                        that need tokens/secrets to run, or the diff contains no modification in code. The collected
                        data are verified, ensuring the logs contained all the necessary information to fix the issue,
                        and the difficulty of the problem is graded. Additional checks are conducted to ensure the
                        benchmark works as intended and a final dataset of 77 items is collected.</p>
                    <h4 id="project-level-code-completion-1">Project-Level Code Completion<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#project-level-code-completion-1" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The Git history of Python projects are scanned to collect commits made after the date 01/01/2022
                        and add new files. The newly added files are extracted and filtered out if they have 200 ≤
                        number of lines ≤ 2,000. Context for each file is gathered from the respective parent commit,
                        saving all the code and text files (such as build files and documentation). This dataset is
                        split into four parts based on the total size of Python files in the repository snapshot. Each
                        entry in the dataset has a list of lines for completion averaging 35 lines. Since there is
                        varying difficulty in completing code lines (<em>“function declaration lines can be challenging
                            due to uncertainty, whereas loop definition can be straightforward”</em>), code lines are
                        classified into six categories. The four parts of the dataset for this task has: 4,686, 8,676,
                        9,631, and 9,810 completion lines.</p>
                    <h4 id="commit-message-generation-1">Commit Message Generation<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#commit-message-generation-1" class="internal"><svg width="18"
                                height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                                stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>For this task, the CommitChronicle dataset is used as the main data source rather than rebuilding
                        a dataset of commits form scratch. A subset is derived from this data source through filtering
                        with criteria such as minimum length in words, message format, etc. Commits with larger changes
                        are prioritised, samples whose diffs exceed 3,000 characters are kept. After a final manual
                        filtering stage, 163 commits from 34 repositories are stored.</p>
                    <h4 id="bug-localisation-1">Bug Localisation<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#bug-localisation-1"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The initial collection of issues and PRs from GitHub are processed to build the dataset for the
                        bug localisation task. PRs are linked with associated issues using regular expression to parse
                        links in PRs’ titles, description comments, and issue comments. Python, Java, and Kotlin are the
                        target languages for this task as this suit the authors’ ability for later manual labelling.
                        From the collected 7,479 pairs of bug issues and corresponding pull requests, 50 data points are
                        manually labelled for each language “<em>to see that they meet the following criteria: the issue
                            description is complete and fully describes the introduced changes, while the changes do
                            indeed fix the issue and do not produce code irrelevant to it.</em>”</p>
                    <h4 id="module-summarization-1">Module Summarization<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true" href="example-benchmarks.htm#module-summarization-1"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>This dataset is constructed from documentation sourced from files identified by their file
                        extension (<code>.md .txt</code>…) existing in a <code>docs</code> folder. Associated code is
                        found by parsing the documentation and extracting links to files and directories with source
                        code. Only documentation files with mentions of source code files are used in subsequent steps.
                        Documentation files are converted into plain text format, removing all other syntax e.g.
                        Markdown, and files with less than 10 lines are removed. A manual review is conducted to ensure
                        that the extracted documentation text accurately summarises the corresponding code and vice
                        versa. Additional steps, such as checking the documentation summarises an entire module/project
                        rather than a specific piece of code, form part of the manual review. Each documentation file is
                        manually assigned an “intent” which describes what the model should aim to produce in the
                        documentation. The final dataset for this task consists of 216 documentation files with
                        associated source code and a specified intent for documentation generation.</p>
                    <h3 id="3-evaluation-metrics-1">3. Evaluation Metrics<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true" href="example-benchmarks.htm#3-evaluation-metrics-1"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <h4 id="library-based-code-generation-2">Library-based code generation:<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#library-based-code-generation-2" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The metric used for this task follow other work on assessing code generation quality. ChrF is
                        employed to measure the similarity between the generated code and the original human-written
                        one. API recall is also measured as the ratio of library-specific API calls in the reference,
                        human-written, solution and the generated one.</p>
                    <h4 id="ci-builds-repair-2">CI Builds Repair:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#ci-builds-repair-2"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Here the metric is simply percentage of successfully fixed builds by the model.</p>
                    <h4 id="project-level-code-completion-2">Project-Level Code Completion:<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#project-level-code-completion-2" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>The main metric for project-level code completion is the exact match of generated lines per
                        category in the task dataset. A prediction is deemed correct if it matches the provided answer
                        in the data after removing leading and trailing whitespaces from both.</p>
                    <h4 id="commit-message-generation-2">Commit Message Generation:<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#commit-message-generation-2" class="internal"><svg width="18"
                                height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                                stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Simpler methods to other works are used as metrics for this task: BLEU, ROUGE, CHrF and BERT<span
                            class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span
                                        class="strut" style="height:0.3014em;vertical-align:-0.15em;"></span><span
                                        class="mord"><span></span><span class="msupsub"><span
                                                class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist"
                                                        style="height:0.1514em;"><span
                                                            style="top:-2.55em;margin-right:0.05em;"><span
                                                                class="pstrut" style="height:2.7em;"></span><span
                                                                class="sizing reset-size6 size3 mtight"><span
                                                                    class="mord mtight"><span
                                                                        class="mord text mtight"><span
                                                                            class="mord mtight">score</span></span></span></span></span></span><span
                                                        class="vlist-s">​</span></span><span class="vlist-r"><span
                                                        class="vlist"
                                                        style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
                    </p>
                    <h4 id="bug-localisation-2">Bug Localisation:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#bug-localisation-2"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Information retrieval metrics are used for the bug localisation task, that is, precision, recall,
                        F1 score, and mean average precision. For changes that require a single file, precision and
                        recall are measured by considering the most probable file returned by the model. For cases where
                        there are more than a single file, the metrics are measured by considering the top 2 recommended
                        files.</p>
                    <h4 id="module-summarization-2">Module Summarization:<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true" href="example-benchmarks.htm#module-summarization-2"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>A new metric is proposed called <em>CompScore</em> which uses an LLM to determine which of the
                        generated or ground truth documentation better explains and matches the code.</p>
                    <h3 id="experiments-1">Experiments<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#experiments-1" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h3>
                    <h4 id="library-based-code-generation-3">Library-based code generation:<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#library-based-code-generation-3" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Two experiments are conducted, one where the models are not given any access to the library and
                        the other where library context is provided using BM-25 retrieval.</p>
                    <h4 id="ci-builds-repair-3">CI Builds Repair:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#ci-builds-repair-3"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>LLMs are instructed with: <em>“Fix CI in order for tests to pass. Relevant logs:
                            {relevant_logs}”</em>. Relevant logs are derived from build logs; the context surrounding
                        the first occurrence of the string “string” are provided (in total 7 lines). Models are prompted
                        to change the code blocks that were edited in the ground-truth fixing diff. The output from the
                        LLM includes edited code sections that are then processed into a diff and returned to the
                        benchmark for testing.</p>
                    <h4 id="project-level-code-completion-3">Project-Level Code Completion:<a role="anchor"
                            aria-hidden="true" tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#project-level-code-completion-3" class="internal"><svg
                                width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                                stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Several strategies for composing the input context from the repository files are tested with
                        details provided in the Appendix of the paper. The best method is building the context from
                        files that are closest in the file tree to the target file.</p>
                    <h4 id="commit-message-generation-3">Commit Message Generation:<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true"
                            href="example-benchmarks.htm#commit-message-generation-3" class="internal"><svg width="18"
                                height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                                stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>No additional methods are employed during experimentation.</p>
                    <h4 id="bug-localisation-3">Bug Localisation:<a role="anchor" aria-hidden="true" tabindex="-1"
                            data-no-popover="true" href="example-benchmarks.htm#bug-localisation-3"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Various retrieval-based approaches are tested for this benchmark task in the paper. Also, GPT 3.5
                        and GPT 4 chat models are tested by prompting them to indicated from one to five bugged files
                        using the issue description and the full list of files from the repository. If the prompt
                        exceeds the context size, the list of files is split into multiple queries, followed by a final
                        query that combines all the outputs and finalises the result.</p>
                    <h4 id="module-summarization-3">Module Summarization:<a role="anchor" aria-hidden="true"
                            tabindex="-1" data-no-popover="true" href="example-benchmarks.htm#module-summarization-3"
                            class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none"
                                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                                <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
                                <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
                            </svg></a></h4>
                    <p>Experiments are conducted within a zero-shot setting. The model uses information about the target
                        file name, intent, and the relevant code for every sample. The generated documentation is
                        compared with the ground-truth provided in the dataset and the <em>CompScore</em> is determined.
                    </p>
                </article>
                <hr />
                <div class="page-footer"></div>
            </div>
            <div class="right sidebar">
                <div class="graph">
                    <h3>Graph View</h3>
                    <div class="graph-outer">
                        <div id="graph-container"
                            data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}">
                        </div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1"
                                xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px"
                                y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve">
                                <path
                                    d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z">
                                </path>
                            </svg></button>
                    </div>
                    <div id="global-graph-outer">
                        <div id="global-graph-container"
                            data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}">
                        </div>
                    </div>
                </div>
                <div class="toc desktop-only"><button type="button" id="toc" class aria-controls="toc-content"
                        aria-expanded="true">
                        <h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24"
                            viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                            stroke-linecap="round" stroke-linejoin="round" class="fold">
                            <polyline points="6 9 12 15 18 9"></polyline>
                        </svg>
                    </button>
                    <div id="toc-content" class>
                        <ul class="overflow">
                            <li class="depth-0"><a
                                    href="example-benchmarks.htm#swe-bench-can-language-models-resolve-real-world-github-issues"
                                    data-for="swe-bench-can-language-models-resolve-real-world-github-issues">SWE-bench:
                                    Can Language Models Resolve Real-World GitHub Issues?</a></li>
                            <li class="depth-1"><a href="example-benchmarks.htm#1-task-selection"
                                    data-for="1-task-selection">1. Task Selection</a></li>
                            <li class="depth-1"><a href="example-benchmarks.htm#2-dataset-creation"
                                    data-for="2-dataset-creation">2. Dataset Creation</a></li>
                            <li class="depth-1"><a href="example-benchmarks.htm#3-evaluation-metrics"
                                    data-for="3-evaluation-metrics">3. Evaluation Metrics</a></li>
                            <li class="depth-1"><a href="example-benchmarks.htm#experiments"
                                    data-for="experiments">Experiments</a></li>
                            <li class="depth-0"><a
                                    href="https://hotpotqa.netlify.app/onboarding/example-benchmarks.htm#long-code-arena-a-set-of-benchmarks-for-long-context-code-models"
                                    data-for="long-code-arena-a-set-of-benchmarks-for-long-context-code-models">Long
                                    Code Arena: a Set of Benchmarks for Long-Context Code Models</a></li>
                            <li class="depth-1"><a
                                    href="https://hotpotqa.netlify.app/onboarding/example-benchmarks.htm#1-task-selection-1"
                                    data-for="1-task-selection-1">1. Task Selection</a></li>
                            <li class="depth-1"><a
                                    href="https://hotpotqa.netlify.app/onboarding/example-benchmarks.htm#2-dataset-creation-1"
                                    data-for="2-dataset-creation-1">2. Dataset Creation</a></li>
                            <li class="depth-1"><a
                                    href="https://hotpotqa.netlify.app/onboarding/example-benchmarks.htm#3-evaluation-metrics-1"
                                    data-for="3-evaluation-metrics-1">3. Evaluation Metrics</a></li>
                            <li class="depth-1"><a
                                    href="https://hotpotqa.netlify.app/onboarding/example-benchmarks.htm#experiments-1"
                                    data-for="experiments-1">Experiments</a></li>
                        </ul>
                    </div>
                </div>
                <div class="backlinks">
                    <h3>Backlinks</h3>
                    <ul class="overflow">
                        <li><a href="https://hotpotqa.netlify.app/Onboarding/homework-task"
                                class="internal">Homework?!?</a></li>
                        <li><a href="https://hotpotqa.netlify.app/Onboarding/intro-to-benchmarking" class="internal">A
                                Brief Introduction to Benchmarking</a></li>
                        <li><a href="https://hotpotqa.netlify.app/hotpotqa_netlify_default.html"
                                class="internal">hotpotqa: A Loooong Context Benchmark</a></li>
                    </ul>
                </div>
            </div>
            <footer class>
                <ul>
                    <li><a href="https://github.com/Siphono-Bench/hotpotqa">GitHub</a></li>
                    <li><a href="https://discord.gg/4TfheqUm">Discord</a></li>
                </ul>
            </footer>
        </div>
    </div>
</body>
<script type="application/javascript">function c() { let t = this.parentElement; t.classList.toggle("is-collapsed"); let l = t.classList.contains("is-collapsed") ? this.scrollHeight : t.scrollHeight; t.style.maxHeight = l + "px"; let o = t, e = t.parentElement; for (; e;) { if (!e.classList.contains("callout")) return; let n = e.classList.contains("is-collapsed") ? e.scrollHeight : e.scrollHeight + o.scrollHeight; e.style.maxHeight = n + "px", o = e, e = e.parentElement } } function i() { let t = document.getElementsByClassName("callout is-collapsible"); for (let s of t) { let l = s.firstElementChild; if (l) { l.addEventListener("click", c), window.addCleanup(() => l.removeEventListener("click", c)); let e = s.classList.contains("is-collapsed") ? l.scrollHeight : s.scrollHeight; s.style.maxHeight = e + "px" } } } document.addEventListener("nav", i); window.addEventListener("resize", i);
</script>
<script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js"
    type="application/javascript"></script>
<script src="../postscript.js" type="module"></script>

</html>